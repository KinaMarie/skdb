cols='80' >The '''s'''ocietal engineering '''k'''nowledge '''d'''atabase ([[skdb]]) is kind of like aptitude, yum, [http://cpan.org/ CPAN], rpm, etc., in that it hosts modules ([[blackbox]]es) for projects, whether digital or physical. These project packages include simulation tools for tweaking, software packages to rewrite the package for a particular use, explicit well-documented links and dependencies to other projects, etc. Skdb started as a need for [[self-replication]]: the idea is to come up with as much social knowledge as possible (stuff that can't be derived from first principles on the spot) and then connect all of the components together and fish out the Hamiltonian paths.





To continue: just as you cannot search for software based on functionality, you cannot search for other tools based on functionality, except in as much as the language and social context allows you to express similiar ideas that others have had before ([[natural languages]] have allowed this to some extent). The success of APT and friends comes not from any magical software search functionality, but rather in the wide-spread social diffusion and the easy accessibility to get new project components, to play with them, modify them and implement them on the spot. Implementation for physical projects will become much easier when we have [[self-replication]] and [[fabbers]] like RepRap, hextatic, etc., and in the mean time we have [[hu]]mans who can sufficiently track down materials and tools (by following specific instructions) to make the [[blackbox]] that they need for a project. 







Part of the database will contain [[biobricks]], or genes and combinations of genes that do particular tasks. Although you cannot specify all possible interactions with all possible other components, you can generally intend for a project to work in a certain way.







Imagine the possibilities for project design now: using a database, you can download different components and just add it into the project, if it doesn't exist yet leave a few comments and hope that somebody can come along and make something to meet your desired specifications. Just leave the &quot;project definition files&quot; up on the net somewhere, and somebody is bound to come around and tweak it and implement it, as long as we can specifically reference other projects, tools, and people that have done the same thing.



= Getting started =

* General file specification standard

* General client (hack Debian APT?)

** fenn's suggested name for the package manager: [[autogenix]].

* Server daemon process

* Need to get a number of servers to start hosting copies of the database

* Seed the database with a number of functional parts, components, tools, etc.

** Basic: [[lemon battery]], [[paper airplane]], ...

** Advanced: [[airplane]], [[automobile]], [[spaceship]], [[computer]] ([[microprocessor]] -- see [http://opencores.org OpenCores]), ...

* '''Note''' that it would be ''nice'' to start with the [[elements]] and specify from the ground up where to go get them (such as the data on [http://mindat.org/ mindat.org]), and then how to process them with the materials around there, but we ''must'' avoid the infinite regress in trying to find everything back down to the source because there's a world of resources and materials already available (though poorly mapped) that can be used to boostrap various projects. So each project, component, part, tool is more like a node that aggregates information about that specific object/concept/idea, while also allowing an interface for simulation or 'making' a final project, or specifications on how to make it if it's generally not available; i.e., this is all 'blackboxed' but with information on what to do and what approaches to take if something is unavailable, not localized, etc.



= Unit testing =

All packages are required to have their own method of doing 'unit tests' to test each of the features. In a project, the programmer should be able to call upon an overall test of all of the units working together (pipelined and whatnot).



= OSCOMAK =

2008-04-06: I see no differences between this project and [http://www.kurtz-fernhout.com/oscomak/index.htm OSCOMAK]. -- [[User:Kanzure|Kanzure]] 18:16, 6 April 2008 (CDT)



= 2008-04-06: File format =

* 2008-04-06: fenn suggested using GNU 'units' (the program) for dealing with unit conversions between different programs.





You generally cannot do a single program that can do everything. Same with file formats. You do not want OSCOMAK/skdb to become a giant object-oriented database of beaurocracy. Instead, you need to be able to take creative license at all of the different nodes/packages, and from there take things where you need it to go. Each package will be a wiki page, much like the CPAN pod entries for perl modules. These entries would specify where to get information and what packages of PDFs to read (or links and pages cached from the web, anything really). Then, there will also be a readout of common programs that are used with that package or relations to other programs. This would be like apt-cache-search, except a full ontology can be given, plus guides for understanding and interpreting the ontologies so that people can find the software that they need. It may be possible to one day have automated software interpret and parse the ontologies to get to the programs they need, but in the mean time we can just let people assemble the software packages together. This is much like &quot;coming to terms with the authors [of a paper]&quot; (a quote I found in a paper I was reading a few weeks ago re: how to read papers). Once a programmer has been sufficiently introduced into a topic, he can then proceed to immediately take advantage of the available software that is mentioned in the package file. Therefore, the file format is more like a way to organize information and knowledge and mention where it was discovered, references, links, ways to get ahold of the responsible individuals, as well as an easy way to query for related software, and then allowing the person to interpret the (and I emphasize) ''natural language'' descriptions of the software packages in order to find the component he needs. These software packages will do various things, depending on the package. For example, in [[computational fluid dynamics]] there are software packages that can do things like [[finite element analysis]] or [[fluid flows]] etc. Those would be examples of programs to add. Their relation to the subject is that they are given a [[mesh file]] and then they take this data and apply a set of specifications [there does not seem to be a standardized format for this yet, so this is perhaps a poor example, but whatever], and then do a simulation, and the simulation can be automatically interpreted if necessary (i.e., does it fall within the mission parameters for the use of the overall system that the programmer is designing?). 





I was worrying about dependency-specification for a while. You would have to specify the fundamental variables that each package is using. This can still be included, a special type of file format (and then other programs can fill in the blanks or do special things to come up with exact materials, if the package is for some specialized subset of all possible materials). But it's not the central focus. The central focus is an easy way to access and query and download (1) societal knowledge and (2) a way to download the software. As part of #1 would be information on how the software in #2 is relevant. So, there should be a querying system (apt-get / [[autogenix]]). The system to download specific &quot;knowledge-sets&quot; (kind of like data-sets) should involve a basic hierarchical navigation system for exploring different perspectives of the ontology of the entire database. There should not be only one, massive encompassing presentation of all of the software packages -- rather, different ontologies should be tagged and accessible by searching for such tags, thus finding possible entrances into the social knowledge by using the right keywords (it's amazing what I have learned over the users by using Google and reading to find new keywords to use ...). On this note, [http://heybryan.org/projects/autoscholar/ AutoScholar] is an excellent example of an interface system too, except that it's geared towards downloading papers and is not necessarily associated with the relevant software. Actually, what if we developed a website that would serve as a ghost-overlay to the scientific publication journals and databases? This would just mean that we categorize the papers within their respective fields, show the social network as well as conceptual network and advancements and so on, and then we provide the relevant software. The knowledge is already there. Just needs to be locally cached and aggregated, and digested into new forms (thus the wiki part).





Theoretically, we can add specifications to a package format to describe the input files and types of input parameters to a software package, so really the 'units' idea should still be applicable. This is a way of using interconnecting software packages so that the flow is not interrupted or lost in the process. Just note that it's not the main function of the system. -- [[User:71.41.149.62|71.41.149.62]] 08:51, 7 April 2008 (CDT)



= 2008-04-08: Thoughts on the structure =

There are two main components that I am particularly concerned about: (1) formalized knowledge in the form of data sets, units, specifications on file formats, mathematical models stored in data files, etc., and then (2) the social knowlede that we process and convert into #1. All that we really need to do is figure out how we want #2 and #1 to work, and then create a certain type of (wikiable) file format to represent a project/unit in the overall database. It does not matter where on the internet the files are, as long as the user can automatically fetch them from the shell. It would be ideal if the database can be mirrored on other locations. It would also be ideal if we can do CVS+wiki all at the same time. What would the namespace structure of the wiki look like? There would be a namespace for package files, and then what? A namespace for the different types of files? It is important to clearly separate &quot;natural language&quot; from everything else. -- [[User:Kanzure|Kanzure]] 15:54, 8 April 2008 (CDT)



tags tags tags! -fenn

: Sure. -- [[User:71.41.149.62|71.41.149.62]] 08:43, 9 April 2008 (CDT)

:: See [http://debtags.alioth.debian.org/ debtags] and a [http://wiki.debian.org/DebTags wiki page on debtags]. -- [[User:Kanzure|Kanzure]] 10:35, 13 April 2008 (CDT)



= 2008-04-09: A pretty good bet on what the start can be =

Okay, I think I know what to start with. The trick is the file formats and specifications for each type of file format. You know the websites where they list file format types and what programs might be used to work with them? Same thing here. Autogenix would be used to download new meta-information on file formats for various programs, with backlinks and references to the programs in the local database. Yesterday I was figuring that an interesting way to implement this would be to include a new way to specify file format IO in the parameters for programs. For example, here's the conventional approach to telling people about the parameters to your program (below). This may be known as '''shell wrapping''' (what I want). What's currently out there are [http://perldoc.perl.org/Pod/Usage.html usage statements (pod2usage here)]. Yeah, the pod2usage module is not enough, it's not formal enough, it's pretty basic and based off of natural language processing (look at the examples on that page) -- instead, we need to formalize this and have a database for managing file formats and such information. For Ruby, [http://cmdline.rubyforge.org/ the cmdline tool] might be the solution - but it looks like more natural language stuff. Okay -- so I can't find any specific examples of it hardcoded, but it's everywhere, and it's not formally identified as such (I had to go start the Wikipedia article on [http://en.wikipedia.org/wiki/Usage_message usage messages]. But a good example might be [http://curl.haxx.se/docs/manpage.html cURL's help page] or [http://en.wikipedia.org/wiki/Manual_page_(Unix) any man page] really. 





What would a more formal API for specifying IO file format types? You would have to reference a centralized database, maybe a [[DTD]] [http://en.wikipedia.org/wiki/DTD] (what's that about?). So this will end up being a simple library or module to include while programming, and then instead of hardcoding some natural language to specify the input parameters, you reference the database, and then you would output a formal file when somebody passes the -h or --help parameter to your program. Plus you'd submit an entry to the database so that there's a reference to your program. You would also tag and write a description of the program (which would have its own file format, a simple XML-like method). The calls to the API should be simple. They should not be runtime calls, but rather compile-time calls --- the idea is that you construct the XML output specifying acceptable IO formats that the current program uses, right? So in this sense it may be silly to do it at compile-time since you may as well open up another file and just make a new file for specifying the data types expected to be passed (this is not the same thing as &quot;error, expected integer and got (noninteger) object instead&quot; - there are certain integer numbers that can be of a certain type, and bash and other shells are not 'typed' systems where you see &quot;error, wget output is not the same as cat output&quot; or whatever). So this would be a secondary tool to run when compiling your projects -- something that could easily be called in a makefile. This does not solve the code-documentation problem ([http://heybryan.org/bookmarking.html]) [not that it was supposed to in the first place]. 





So once we have the file format database, we can then start making up our specifications for each of the file formats plus let [[autogenix]] download new scripts to interpret new versions of the files whenever necessary. Then, we just throw up the packages on the wiki, with file format specifications at the top (kind of like the XHTML version specification number at the top of w3c valid HTML docs). The wiki does not need separate namespaces for different aspects of packages, just solid, explicit, meta-information on the relationship between two pages, i.e. the page [[file.pack]] might cite some reference information which can be downloaded at [[filepack-information.html]] -- and instead of just interpreting the file extension (the previous method), read the file version/format specs to go lookup software that can handle that sort of data. When we want to compress a package and all of its contents, we can just read the [[file.pack]] page and then scan its references and 'links' to other packages and download it all, so it's not like everything will be all over the place, as long as we do routine backups and mirroring etc. -- [[User:71.41.149.62|71.41.149.62]] 08:43, 9 April 2008 (CDT)



= 2008-04-13: ikiwiki =

For the wiki+versioning system, see [http://ikiwiki.info/ ikiwiki] and [http://ikiwiki.info/rcs/git/ git]. Now on to the package manager, [[autogenix]]. The linux package managers are usually broken into two main tools: dpkg for [http://debian.org/ debian] or emerge for [http://gentoo.org/ gentoo], which are the low-level packaging tools for dealing with packages on a client/user machine. Then, there are interfaces like apt, which connect to package repositories (as attained by sources.list) to get all of the metadata on all of the packages at once (it's not much data, so it's not that much of a load on the system - 10 MB compressed?).

</textarea>

